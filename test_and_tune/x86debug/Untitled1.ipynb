{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "outstanding-customer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "from tvm import topi\n",
    "from tvm import autotvm\n",
    "#from ...intrin import *\n",
    "\n",
    "from tvm.topi.nn.pad import pad\n",
    "from tvm.topi.nn.utils import get_pad_tuple\n",
    "from tvm.topi.utils import simplify, get_const_tuple\n",
    "import math\n",
    "out_dtype=dtype=\"float32\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graphic-vermont",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............initial after defition in compute part:\t \n",
      "  primfn(Input_1: handle, Filter_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {Input: Buffer(Input_2: Pointer(float32), float32, [1, 128, 38, 38], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [1, 64, 36, 36], []),\n",
      "             Filter: Buffer(Filter_2: Pointer(float32), float32, [64, 128, 3, 3], [])}\n",
      "  buffer_map = {Input_1: Input, Filter_1: Filter, compute_1: compute} {\n",
      "  for (c_out: int32, 0, 64) {\n",
      "    for (h: int32, 0, 36) {\n",
      "      for (w: int32, 0, 36) {\n",
      "        compute_2[(((c_out*1296) + (h*36)) + w)] = 0f32\n",
      "        for (rc: int32, 0, 128) {\n",
      "          for (ry: int32, 0, 3) {\n",
      "            for (rx: int32, 0, 3) {\n",
      "              compute_2[(((c_out*1296) + (h*36)) + w)] = ((float32*)compute_2[(((c_out*1296) + (h*36)) + w)] + ((float32*)Input_2[(((((rc*1444) + (h*38)) + (ry*38)) + w) + rx)]*(float32*)Filter_2[((((c_out*1152) + (rc*9)) + (ry*3)) + rx)]))\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#switch for print stmt and debug\n",
    "DEBUG=1\n",
    "STRIPE_LEN = 16\n",
    "TBATCH = 1\n",
    "TIC = 16\n",
    "TOC = 16\n",
    "\n",
    "#prepare data for Input and Filtr\n",
    "shape_Input = (1,128,38,38)\n",
    "#[passed] shape_Input = (1,128,3,38)\n",
    "Input = te.placeholder(shape_Input, name=\"Input\",dtype=dtype)\n",
    "shape_Kernel = (64,128,3,3)\n",
    "Filter =  te.placeholder(shape_Kernel, name=\"Filter\",dtype=dtype)\n",
    "stride=(1,1)\n",
    "padding=(0,0)\n",
    "dilation=(1,1)\n",
    "dilation_h,dilation_w = dilation\n",
    "stride_h,stride_w =stride\n",
    "batch, in_channel, in_height, in_width = Input.shape\n",
    "num_filter, channel, kernel_h, kernel_w = Filter.shape\n",
    "# compute the output shape\n",
    "dilated_kernel_h = (kernel_h - 1) * dilation_h + 1\n",
    "dilated_kernel_w = (kernel_w - 1) * dilation_w + 1\n",
    "pad_top, pad_left, pad_down, pad_right = get_pad_tuple(\n",
    "    padding, (dilated_kernel_h, dilated_kernel_w))\n",
    "out_channel = num_filter\n",
    "out_height = simplify((in_height - dilated_kernel_h + pad_top + pad_down) // stride_h + 1)\n",
    "out_width = simplify((in_width - dilated_kernel_w + pad_left + pad_right) // stride_w + 1)\n",
    "# compute graph\n",
    "pad_before = [0, 0, pad_top, pad_left]\n",
    "pad_after = [0, 0, pad_down, pad_right]\n",
    "#padding or not\n",
    "#temp = pad(Input, pad_before, pad_after, name=\"pad_temp\")\n",
    "rc = te.reduce_axis((0, in_channel), name='rc')\n",
    "ry = te.reduce_axis((0, kernel_h), name='ry')\n",
    "rx = te.reduce_axis((0, kernel_w), name='rx')\n",
    "shape_output = (batch, out_channel, out_height, out_width),\n",
    "ofm = te.compute(\n",
    "    (batch, out_channel, out_height, out_width),\n",
    "    lambda n, c_out, h, w: te.sum(\n",
    "        Input[n, rc, h * stride_h + ry * dilation_h,\n",
    "            w * stride_w + rx * dilation_w].astype(out_dtype) *\n",
    "        Filter[c_out, rc, ry, rx].astype(out_dtype),\n",
    "        axis=[rc, ry, rx]), tag=\"conv2d_nchw\",attrs={'stride': stride, 'padding': padding, 'dilation': dilation})\n",
    "\n",
    "if DEBUG:\n",
    "    sch_global = te.create_schedule(ofm.op)\n",
    "    stmt = tvm.lower(sch_global,[Input,Filter,ofm],simple_mode=True)\n",
    "    print(\".............initial after defition in compute part:\\t \\n \",stmt,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "verified-courtesy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dom_map-1 before schedule]:\t\n",
      " {iter_var(ry, range(min=0, ext=3)): range(min=0, ext=3), iter_var(n, range(min=0, ext=1)): range(min=0, ext=1), iter_var(h, range(min=0, ext=36)): range(min=0, ext=36), iter_var(rx, range(min=0, ext=3)): range(min=0, ext=3), iter_var(w, range(min=0, ext=36)): range(min=0, ext=36), iter_var(rc, range(min=0, ext=128)): range(min=0, ext=128), iter_var(c_out, range(min=0, ext=64)): range(min=0, ext=64)} \n",
      "\n",
      "..........after fuse && split && reorder ......dom_map:  dom_map-1-2:\n",
      " {iter_var(h.w.fused.inner, ): range(min=0, ext=16), iter_var(h.w.fused, ): range(min=0, ext=1296), iter_var(ry, range(min=0, ext=3)): range(min=0, ext=3), iter_var(c_out.inner, ): range(min=0, ext=16), iter_var(n, range(min=0, ext=1)): range(min=0, ext=1), iter_var(h.w.fused.outer, ): range(min=0, ext=81), iter_var(h, range(min=0, ext=36)): range(min=0, ext=36), iter_var(w, range(min=0, ext=36)): range(min=0, ext=36), iter_var(c_out.outer, ): range(min=0, ext=4), iter_var(rx, range(min=0, ext=3)): range(min=0, ext=3), iter_var(rc, range(min=0, ext=128)): range(min=0, ext=128), iter_var(c_out, range(min=0, ext=64)): range(min=0, ext=64)} \n",
      "\n",
      "...........after [oh,ow] fuse && split[Ntohow,tohow] [Ntoc,toc]  && reorder (n,Ntoc, Ntohow,tohow,toc,ic,kh,kw).....lower stmt:  primfn(Input_1: handle, Filter_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {Input: Buffer(Input_2: Pointer(float32), float32, [1, 128, 38, 38], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [1, 64, 36, 36], []),\n",
      "             Filter: Buffer(Filter_2: Pointer(float32), float32, [64, 128, 3, 3], [])}\n",
      "  buffer_map = {Input_1: Input, Filter_1: Filter, compute_1: compute} {\n",
      "  for (c_out.outer: int32, 0, 4) {\n",
      "    for (h.w.fused.outer: int32, 0, 81) {\n",
      "      for (h.w.fused.inner: int32, 0, 16) {\n",
      "        for (c_out.inner: int32, 0, 16) {\n",
      "          compute_2[((((c_out.outer*20736) + (c_out.inner*1296)) + (h.w.fused.outer*16)) + h.w.fused.inner)] = 0f32\n",
      "          for (rc: int32, 0, 128) {\n",
      "            for (ry: int32, 0, 3) {\n",
      "              for (rx: int32, 0, 3) {\n",
      "                compute_2[((((c_out.outer*20736) + (c_out.inner*1296)) + (h.w.fused.outer*16)) + h.w.fused.inner)] = ((float32*)compute_2[((((c_out.outer*20736) + (c_out.inner*1296)) + (h.w.fused.outer*16)) + h.w.fused.inner)] + ((float32*)Input_2[(((((rc*1444) + (floordiv(((h.w.fused.outer*16) + h.w.fused.inner), 36)*38)) + (ry*38)) + rx) + floormod(((h.w.fused.outer*16) + h.w.fused.inner), 36))]*(float32*)Filter_2[(((((c_out.outer*18432) + (c_out.inner*1152)) + (rc*9)) + (ry*3)) + rx)]))\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "https://discuss.te.ai/t/tensorize-with-stride-for-input-tensors/6018\n",
    "[tqchen]\n",
    "By default the tensor buffer declaration requires a compact buffer,\n",
    "that means that the tensorized region need to be contiguous.\n",
    "To relax the constraint, you can declare a buffer with symbolic strides\n",
    "when declaring the tensor intrin, of course your low level instruction\n",
    "must also support strided matrices as an input\n",
    "翻译：\n",
    "默认情况下，张量缓冲区声明需要一个紧凑缓冲区，\n",
    "意思是说，待tensorize的区域必须是连续的。\n",
    "要解除这种约束，在声明张量内因时，可以声明一个带有符号跨度的buffer,例如，ww = te.var(\"ww\")\n",
    "当然是您的底层指令还必须支持跨步矩阵作为输入\n",
    "How to use tensorize https://discuss.te.ai/t/how-to-use-tensorize/424\n",
    "There are two cases here.\n",
    "1, describe the compute logic, we can call it original compute\n",
    "2, you need to do some schedule (split) to expose the axis you wanna tensorize, and mark it with tensorize api\n",
    "3, you need to describe intrinsic pattern, which includes two parts, one is a clone of original compute, another is the intrinsic pattern you wanna use\n",
    "4, after that, in scheduleOps, tensorize will try to do pattern match for original compute and clone compute, if success, will replace the marked axis with intrinsic.\n",
    "'''\n",
    "def intrin_partial_conv2d_ohow_toc(outs,shape_data,shape_kernel,tohow,toc):\n",
    "    data_intrin = te.placeholder(shape_data, name='data')\n",
    "    kernel_intrin = te.placeholder(shape_kernel, name='kernel')\n",
    "    batch, in_channel, _, _ = shape_data #[1,128,38,38]\n",
    "    num_filter, _, kernel_h, kernel_w = shape_kernel # [16,128,3,3]\n",
    "\n",
    "    ry = te.reduce_axis((0, kernel_h), name='ry')\n",
    "    rx = te.reduce_axis((0, kernel_w), name='rx')\n",
    "    rc = te.reduce_axis((0, in_channel), name='rc')\n",
    "\n",
    "    #shape_out = [1,toc,1,tohow] #[1,16,1,36]\n",
    "    ofm_intrin = te.compute(\n",
    "        (batch,num_filter,1,tohow),\n",
    "        lambda n, c, h, w: te.sum(\n",
    "            data_intrin[n, rc, h * stride_h + ry * dilation_h,\n",
    "                w * stride_w + rx * dilation_w].astype(out_dtype) *\n",
    "            kernel_intrin[c, rc, ry, rx].astype(out_dtype),\n",
    "            axis=[rc, ry, rx]), tag=\"conv2d_nchw\",attrs={'stride': stride, 'padding': padding, 'dilation': dilation})\n",
    "    if DEBUG:\n",
    "        schedule_intrin= te.create_schedule(ofm_intrin.op)\n",
    "        dom_map = te.schedule.InferBound(schedule_intrin)\n",
    "        print(\"[dom_map] in intrinsic:\\t\\n\",dom_map,\"\\n\")\n",
    "\n",
    "    nn, cc , hh, ww = te.var(\"nn\"), te.var(\"cc\"), te.var(\"hh\"), te.var(\"ww\")\n",
    "\n",
    "    #ww = te.floordiv((ww*36),36)\n",
    "    tt = te.floordiv((te.floormod((ww*16), 36) + 15), 36)\n",
    "\n",
    "    data_buf = tvm.tir.decl_buffer(data_intrin.shape, data_intrin.dtype,\n",
    "                        name=\"DATA\",\n",
    "                        offset_factor=1 ,strides=[nn, cc,hh,ww])\n",
    "\n",
    "\n",
    "    kernel_buf = tvm.tir.decl_buffer(kernel_intrin.shape, kernel_intrin.dtype,\n",
    "                        name=\"KERNEL\",\n",
    "                        offset_factor=1 )\n",
    "\n",
    "    ofm_nn, ofm_cc ,ofm_hh, ofm_ww = te.var(\"ofm_nn\"), te.var(\"ofm_cc\"), te.var(\"ofm_hh\"), te.var(\"ofm_ww\")\n",
    "    ofm_buf = tvm.tir.decl_buffer(ofm_intrin.shape, ofm_intrin.dtype,\n",
    "                    name=\"OFM\",\n",
    "                    offset_factor=1,strides=[ofm_cc, ofm_nn,ofm_hh,ofm_ww])\n",
    "\n",
    "                    #,strides=[1,64,36,36])\n",
    "    if DEBUG:\n",
    "        sch_temp = te.create_schedule(ofm_intrin.op)\n",
    "        temp_n,temp_c,temp_h,temp_w = sch_temp[ofm_intrin].op.axis\n",
    "        sch_temp[ofm_intrin].reorder( temp_n,temp_w,temp_h,temp_c)\n",
    "\n",
    "        stmt_temp=tvm.lower(sch_temp,[data_intrin,kernel_intrin,ofm_intrin],simple_mode=True)\n",
    "        print(\"..........intrin_partial_conv2d stmt 【inside of definition 】.......\",stmt_temp,\"\\n\")\n",
    "\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        ib.emit(tvm.tir.stmt.stmt_seq(\n",
    "                tvm.tir.call_extern(\"int32\", \"SailStartCommand\", \"cdma\")))\n",
    "        return ib.get()\n",
    "\n",
    "    def intrin_func2(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        aa, bb = ins\n",
    "        cc = outs[0]\n",
    "        print(\".............intrin_fun2......:\\n\",aa.strides, bb.strides, cc.strides)\n",
    "        ib.emit(tvm.tir.call_extern(\"int32\", \"gemv_update\",\n",
    "                                cc.access_ptr(\"r\"),\n",
    "                                aa.access_ptr(\"r\"),\n",
    "                                bb.access_ptr(\"r\"),\n",
    "                                cc.access_ptr(\"w\")))\n",
    "        # ib.emit(tvm.call_extern(outs[0].dtype, 'vadd', ins[0].access_ptr(\"r\"), ins[1].access_ptr('r'), outs[0].access_ptr('wr')))\n",
    "        return ib.get()\n",
    "\n",
    "\n",
    "        #【PASS】\n",
    "        #return tvm.decl_tensor_intrin(ofm_intrin.op, intrin_func2,binds={data_intrin: data_buf, kernel_intrin: kernel_buf, ofm_intrin: ofm_buf}, name=\"sp_conv2d\")\n",
    "    return te.decl_tensor_intrin(ofm_intrin.op, intrin_func,binds={data_intrin: data_buf, kernel_intrin: kernel_buf, ofm_intrin: ofm_buf}, name=\"sp_conv2d\")\n",
    "\n",
    "dom_map = te.schedule.InferBound(sch_global)\n",
    "print(\"[dom_map-1 before schedule]:\\t\\n\",dom_map,\"\\n\")\n",
    "n,oc,oh,ow = sch_global[ofm].op.axis\n",
    "ic, kh, kw = sch_global[ofm].op.reduce_axis\n",
    "\n",
    "N,IC,IH,IW = Input.shape\n",
    "KN,KC,KH,KW = Filter.shape\n",
    "\n",
    "n_op,oc,oh,ow = ofm.op.axis\n",
    "\n",
    "\n",
    "ohow = sch_global[ofm].fuse(oh, ow)\n",
    "Ntohow, tohow = sch_global[ofm].split(ohow, factor=STRIPE_LEN)\n",
    "Ntoc,toc = sch_global[ofm].split(oc, factor = TOC)# [,16]\n",
    "#Ntic, tic = sch_global[ofm].split(ic, factor = TIC)\n",
    "sch_global[ofm].reorder(n,Ntoc, Ntohow,tohow,toc,ic,kh,kw)\n",
    "if DEBUG:\n",
    "    dom_map = te.schedule.InferBound(sch_global)\n",
    "    print(\"..........after fuse && split && reorder ......dom_map:  dom_map-1-2:\\n\",dom_map,\"\\n\")\n",
    "    temp_stmt3 = tvm.lower(sch_global, (Input, Filter,ofm),simple_mode=True)\n",
    "    print(\"...........after [oh,ow] fuse && split[Ntohow,tohow] [Ntoc,toc]  && reorder (n,Ntoc, Ntohow,tohow,toc,ic,kh,kw).....lower stmt: \",temp_stmt3,\"\\n\")\n",
    "\n",
    "# prepare for tensorize\n",
    "shape_data_intrinsic =[ N,IC,KH,38] #[1,128,3,38]\n",
    "#shape_data = [1,128,3,38]\n",
    "\n",
    "shape_kernel_intrinsic = [TOC,IC,KH,KW] #[16,128,3,3]\n",
    "#shape_kernel = [16,128,3,3]\n",
    "#ideal output [1,16,1,36 ]\n",
    "# impl tensorize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minimal-practitioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dom_map] in intrinsic:\t\n",
      " {iter_var(ry, range(min=0, ext=3)): range(min=0, ext=3), iter_var(rx, range(min=0, ext=3)): range(min=0, ext=3), iter_var(h, range(min=0, ext=1)): range(min=0, ext=1), iter_var(n, range(min=0, ext=1)): range(min=0, ext=1), iter_var(rc, range(min=0, ext=128)): range(min=0, ext=128), iter_var(w, range(min=0, ext=16)): range(min=0, ext=16), iter_var(c, range(min=0, ext=16)): range(min=0, ext=16)} \n",
      "\n",
      "..........intrin_partial_conv2d stmt 【inside of definition 】....... primfn(data_1: handle, kernel_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {data: Buffer(data_2: Pointer(float32), float32, [1, 128, 3, 38], []),\n",
      "             kernel: Buffer(kernel_2: Pointer(float32), float32, [16, 128, 3, 3], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [1, 16, 1, 16], [])}\n",
      "  buffer_map = {data_1: data, kernel_1: kernel, compute_1: compute} {\n",
      "  for (w: int32, 0, 16) {\n",
      "    for (c: int32, 0, 16) {\n",
      "      compute_2[((c*16) + w)] = 0f32\n",
      "      for (rc: int32, 0, 128) {\n",
      "        for (ry: int32, 0, 3) {\n",
      "          for (rx: int32, 0, 3) {\n",
      "            compute_2[((c*16) + w)] = ((float32*)compute_2[((c*16) + w)] + ((float32*)data_2[((((rc*114) + (ry*38)) + w) + rx)]*(float32*)kernel_2[((((c*1152) + (rc*9)) + (ry*3)) + rx)]))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      " \n",
      "\n",
      "......................cutting line .........................\n",
      "\n"
     ]
    },
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  File \"D:\\work\\llvmsrc\\tvm\\src\\tir\\transforms\\arg_binder.cc\", line 40\nTVMError: Bind have an unmet assertion: (bool)0,  on argument OFM.shape[3]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-47f1f7e22c23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"......................cutting line .........................\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mlowered_stmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msch_global\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mInput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFilter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mofm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlowered_stmt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\llvmsrc\\tvm\\python\\tvm\\driver\\build_module.py\u001b[0m in \u001b[0;36mlower\u001b[1;34m(sch, args, name, binds, simple_mode)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m     \u001b[0moptimize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpass_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 245\u001b[1;33m     \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\llvmsrc\\tvm\\python\\tvm\\ir\\transform.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, mod)\u001b[0m\n\u001b[0;32m    125\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mupdated\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mapplying\u001b[0m \u001b[0mthis\u001b[0m \u001b[1;32mpass\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m         \"\"\"\n\u001b[1;32m--> 127\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_ffi_transform_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRunPass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\llvmsrc\\tvm\\python\\tvm\\_ffi\\_ctypes\\packed_func.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         ):\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTVMError\u001b[0m: Traceback (most recent call last):\n  File \"D:\\work\\llvmsrc\\tvm\\src\\tir\\transforms\\arg_binder.cc\", line 40\nTVMError: Bind have an unmet assertion: (bool)0,  on argument OFM.shape[3]"
     ]
    }
   ],
   "source": [
    "partial_conv2d = intrin_partial_conv2d_ohow_toc(ofm,shape_data_intrinsic,shape_kernel_intrinsic,tohow=STRIPE_LEN, toc=TOC)\n",
    "sch_global[ofm].tensorize(tohow, partial_conv2d)\n",
    "\n",
    "if not DEBUG:\n",
    "    sch_global = sch_global.normalize()\n",
    "    dom_map = te.schedule.InferBound(sch_global)\n",
    "    print(\"[after tensorize dom_map-2]:\\t\\n\",dom_map,\"\\n\")\n",
    "    finfer = tvm.get_global_func(\"test.op.InferTensorizeRegion\")\n",
    "    out_dom, in_dom = finfer(sch_global[ofm], dom_map)\n",
    "\n",
    "    print(\"[output_dom]:\\t\\n\",out_dom,\"\\n\")\n",
    "    print(\"[input_dom]:\\t\\n\",in_dom,\"\\n\")\n",
    "\n",
    "    fmatch = tvm.get_global_func(\"test.op.MatchTensorizeBody\")\n",
    "    body = fmatch(sch_global[ofm], out_dom, in_dom, partial_conv2d)\n",
    "\n",
    "    print(\"body[0]:\\t\",tvm.arith.Analyzer().canonical_simplify(body[0]),\"\\n\")\n",
    "    print(\"partial_conv2d.op.body[0]:\\t\",tvm.arith.Analyzer().canonical_simplify(partial_conv2d.op.body[0]),\"\\n\")\n",
    "    #assert tvm.ir_pass.Equal(tvm.arith.Analyzer().canonical_simplify(body[0]),\n",
    "    #                    tvm.arith.Analyzer().canonical_simplify(partial_conv2d.op.body[0]))\n",
    "    print(\"***********Success on body[0] vs partial_conv2d.op.body[0]*************\")\n",
    "\n",
    "print(\"......................cutting line .........................\\n\")\n",
    "lowered_stmt = tvm.lower(sch_global, (Input, Filter,ofm), simple_mode=True)\n",
    "print(lowered_stmt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-pressing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-influence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "micro-transcription",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm-buildkernel",
   "language": "python",
   "name": "tvm-buildkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
