{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "trying-seeker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {B: Buffer(B_2: Pointer(float32), float32, [512, 64], []),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [1024, 64], []),\n",
      "             C: Buffer(C_2: Pointer(float32), float32, [1024, 512], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  for (i: int32, 0, 1024) {\n",
      "    for (j: int32, 0, 512) {\n",
      "      C_2[((i*512) + j)] = 0f32\n",
      "      for (k: int32, 0, 64) {\n",
      "        C_2[((i*512) + j)] = ((float32*)C_2[((i*512) + j)] + ((float32*)A_2[((i*64) + k)]*(float32*)B_2[((j*64) + k)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, print_function\n",
    "\n",
    "import tvm\n",
    "from tvm import te\n",
    "import tvm.testing\n",
    "import numpy as np\n",
    "N, M, L = 1024, 512, 64\n",
    "A = te.placeholder((N, L), name=\"A\")\n",
    "B = te.placeholder((M, L), name=\"B\")\n",
    "k = te.reduce_axis((0, L), name=\"k\")\n",
    "C = te.compute((N, M), lambda i, j: te.sum(A[i, k] * B[j, k], axis=k), name=\"C\")\n",
    "s = te.create_schedule(C.op)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "burning-porter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "primfn(A_1: handle, B_1: handle, C_1: handle) -> ()\n",
      "  attr = {\"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {C: Buffer(C_2: Pointer(float32), float32, [1024, 512], []),\n",
      "             A: Buffer(A_2: Pointer(float32), float32, [1024, 64], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [512, 64], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, C_1: C} {\n",
      "  for (i: int32, 0, 1024) {\n",
      "    for (j.outer: int32, 0, 32) {\n",
      "      for (j.inner: int32, 0, 16) {\n",
      "        C_2[(((i*512) + (j.outer*16)) + j.inner)] = 0f32\n",
      "        for (k: int32, 0, 64) {\n",
      "          C_2[(((i*512) + (j.outer*16)) + j.inner)] = ((float32*)C_2[(((i*512) + (j.outer*16)) + j.inner)] + ((float32*)A_2[((i*64) + k)]*(float32*)B_2[(((j.outer*1024) + (j.inner*64)) + k)]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "factor = 16\n",
    "x, y = C.op.axis\n",
    "(z,) = C.op.reduce_axis\n",
    "yo, yi = s[C].split(y, factor=factor)\n",
    "s[C].reorder(x, yo, yi, z)\n",
    "print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swedish-queensland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "double-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def intrin_gemv(m, l):\n",
    "    a = te.placeholder((l,), name=\"a\")\n",
    "    b = te.placeholder((m, l), name=\"b\")\n",
    "    k = te.reduce_axis((0, l), name=\"k\")\n",
    "    c = te.compute((m,), lambda i: te.sum(a[k] * b[i, k], axis=k), name=\"c\")\n",
    "    Ab = tvm.tir.decl_buffer(a.shape, a.dtype, name=\"A\", offset_factor=1, strides=[1])\n",
    "    Bb = tvm.tir.decl_buffer(b.shape, b.dtype, name=\"B\", offset_factor=1, strides=[te.var(\"s1\"), 1])\n",
    "    Cb = tvm.tir.decl_buffer(c.shape, c.dtype, name=\"C\", offset_factor=1, strides=[1])\n",
    "\n",
    "    def intrin_func(ins, outs):\n",
    "        ib = tvm.tir.ir_builder.create()\n",
    "        aa, bb = ins\n",
    "        cc = outs[0]\n",
    "        ib.emit(\n",
    "            tvm.tir.call_extern(\n",
    "                \"int32\",\n",
    "                \"gemv_update\",\n",
    "                cc.access_ptr(\"w\"),\n",
    "                aa.access_ptr(\"r\"),\n",
    "                bb.access_ptr(\"r\"),\n",
    "                m,\n",
    "                l,\n",
    "                bb.strides[0],\n",
    "            )\n",
    "        )\n",
    "        return ib.get()\n",
    "\n",
    "    return te.decl_tensor_intrin(c.op, intrin_func, binds={a: Ab, b: Bb, c: Cb})\n",
    "def gemv_impl():\n",
    "    cc_code = \"\"\"\n",
    "      extern \"C\" int gemv_update(float *cc, float *aa, float *bb, int m, int l, int stride) {\n",
    "        for (int i = 0; i < m; ++i) {\n",
    "            for (int j = 0; j < l; ++j) {\n",
    "                cc[i] += aa[j] * bb[i * stride + j];\n",
    "            }\n",
    "        }\n",
    "        return 0;\n",
    "      }\n",
    "    \"\"\"\n",
    "    from tvm.contrib import utils, clang\n",
    "\n",
    "    temp = utils.tempdir()\n",
    "    ll_path = temp.relpath(\"temp.ll\")\n",
    "    # Create LLVM ir from c source code\n",
    "    ll_code = clang.create_llvm(cc_code, output=ll_path)\n",
    "    return ll_code\n",
    "\n",
    "gemv = intrin_gemv(factor, L)\n",
    "s[C].tensorize(yi, gemv)\n",
    "#print(tvm.lower(s, [A, B, C], simple_mode=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "renewable-senegal",
   "metadata": {},
   "outputs": [],
   "source": [
    "s[C].pragma(x, \"import_llvm\", gemv_impl())\n",
    "#print(tvm.lower(s, [A, B, C], simple_mode=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dramatic-spoke",
   "metadata": {},
   "outputs": [],
   "source": [
    "func = tvm.build(s, [A, B, C], target=\"llvm\", name=\"gemv\")\n",
    "\n",
    "from tvm.topi.utils import get_const_tuple\n",
    "\n",
    "dtype = A.dtype\n",
    "ctx = tvm.context(\"cpu\", 0)\n",
    "a = np.random.uniform(size=get_const_tuple(A.shape)).astype(dtype)\n",
    "b = np.random.uniform(size=get_const_tuple(B.shape)).astype(dtype)\n",
    "c = tvm.nd.array(np.zeros(get_const_tuple(C.shape), dtype=dtype), ctx)\n",
    "func(tvm.nd.array(a, ctx), tvm.nd.array(b, ctx), c)\n",
    "tvm.testing.assert_allclose(c.asnumpy(), np.dot(a, b.T), rtol=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-native",
   "metadata": {},
   "outputs": [
    {
     "ename": "TVMError",
     "evalue": "Traceback (most recent call last):\n  File \"D:\\work\\llvmsrc\\tvm\\src\\te\\operation\\tensorize.cc\", line 336\nTVMError: \n---------------------------------------------------------------\nAn internal invariant was violated during the execution of TVM.\nPlease read TVM's error reporting guidelines.\nMore details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.\n---------------------------------------------------------------\n  Check failed: expr_equal(lhs, rhs) == false: Failed to match the compute with TensorIntrin Matmul's declaration  provided= reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[(placeholder[0, k]*placeholder[0, k])], init=[], axis=[iter_var(k, range(min=0, ext=128))], where=(bool)1, value_index=0), intrin=  reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[(placeholder[i, k]*placeholder[0, k])], init=[], axis=[iter_var(k, range(min=0, ext=128))], where=(bool)1, value_index=0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTVMError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c0e51beac54b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[0mtest_conv2d_nchw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-c0e51beac54b>\u001b[0m in \u001b[0;36mtest_conv2d_nchw\u001b[1;34m()\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtest_conv2d_nchw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m     \u001b[0mverify_conv2d_nchw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_channel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_height\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0min_width\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_filter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-c0e51beac54b>\u001b[0m in \u001b[0;36mverify_conv2d_nchw\u001b[1;34m(batch, in_channel, in_height, in_width, num_filter, kernel, stride, padding)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\llvmsrc\\tvm\\python\\tvm\\driver\\build_module.py\u001b[0m in \u001b[0;36mlower\u001b[1;34m(sch, args, name, binds, simple_mode)\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;31m# Phase 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSchedule\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 202\u001b[1;33m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mform_irmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    203\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[0mmod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\llvmsrc\\tvm\\python\\tvm\\driver\\build_module.py\u001b[0m in \u001b[0;36mform_irmodule\u001b[1;34m(sch, args, name, binds)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[0msch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[0mbounds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInferBound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m     \u001b[0mstmt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScheduleOps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[0mcompact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mschedule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVerifyCompactBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstmt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\work\\llvmsrc\\tvm\\python\\tvm\\_ffi\\_ctypes\\packed_func.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    235\u001b[0m             \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         ):\n\u001b[1;32m--> 237\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    238\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTVMError\u001b[0m: Traceback (most recent call last):\n  File \"D:\\work\\llvmsrc\\tvm\\src\\te\\operation\\tensorize.cc\", line 336\nTVMError: \n---------------------------------------------------------------\nAn internal invariant was violated during the execution of TVM.\nPlease read TVM's error reporting guidelines.\nMore details can be found here: https://discuss.tvm.ai/t/error-reporting/7793.\n---------------------------------------------------------------\n  Check failed: expr_equal(lhs, rhs) == false: Failed to match the compute with TensorIntrin Matmul's declaration  provided= reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[(placeholder[0, k]*placeholder[0, k])], init=[], axis=[iter_var(k, range(min=0, ext=128))], where=(bool)1, value_index=0), intrin=  reduce(combiner=comm_reducer(result=[(x + y)], lhs=[x], rhs=[y], identity_element=[0f]), source=[(placeholder[i, k]*placeholder[0, k])], init=[], axis=[iter_var(k, range(min=0, ext=128))], where=(bool)1, value_index=0)"
     ]
    }
   ],
   "source": [
    "import tvm\n",
    "from tvm import te\n",
    "\n",
    "def compute_conv2d(A, W, stride, padding):\n",
    "    batch_size, in_channel, height, width = A.shape\n",
    "    out_channel, _ = W.shape\n",
    "\n",
    "    kh = 1\n",
    "    kw = 1\n",
    "\n",
    "    out_height = (height + 2 * padding - kh) // stride + 1\n",
    "    out_width = (width + 2 * padding - kw) // stride + 1\n",
    "\n",
    "    A = te.compute((batch_size, height, width, in_channel), lambda n, h, w, c: A[n, c, h, w])\n",
    "\n",
    "    # convolution\n",
    "    oshape = (batch_size, out_channel, out_height, out_width)\n",
    "\n",
    "    ic = te.reduce_axis((0, in_channel), name='ic')\n",
    "\n",
    "    conv = te.compute(oshape, lambda n, oc, oh, ow:\n",
    "                       te.sum(A[n, oh*stride+kh, ow*stride+kw, ic] * W[oc, ic],\n",
    "                               axis=[ic]),\n",
    "                       name='conv2d', tag=\"conv2d\")\n",
    "    return conv\n",
    "\n",
    "\n",
    "def matmul():\n",
    "    wgt = te.placeholder((1, 128))\n",
    "    inp = te.placeholder((16, 128))\n",
    "\n",
    "    k = te.reduce_axis((0, 128), name=\"k\")\n",
    "\n",
    "    out = te.compute((16, 1),\n",
    "                      lambda i, j: te.sum(inp(i, k) * wgt(j, k), axis=[k]))\n",
    "\n",
    "    def intrin_func(inputs, outputs):\n",
    "        def body():\n",
    "            irb = tvm.tir.ir_builder.create()\n",
    "            irb.emit(tvm.tir.call_extern(\n",
    "                \"float32\", \"Matmul\"))\n",
    "            return irb.get()\n",
    "\n",
    "        def reset():\n",
    "            return body()\n",
    "\n",
    "        def update():\n",
    "            return body()\n",
    "\n",
    "        return body(), reset(), update()\n",
    "\n",
    "    return te.decl_tensor_intrin(out.op, intrin_func, name=\"Matmul\")\n",
    "\n",
    "\n",
    "def schedule_conv2d(out):\n",
    "    s = te.create_schedule(out.op)\n",
    "    conv = out.op.output(0)\n",
    "    data, kernel = conv.op.input_tensors\n",
    "\n",
    "    batch, oc, oh, ow = s[conv].op.axis\n",
    "    ic, = s[conv].op.reduce_axis\n",
    "\n",
    "    s[conv].tensorize(ow, matmul())\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def verify_conv2d_nchw(batch, in_channel, in_height, in_width, num_filter, kernel, stride, padding):\n",
    "    A = te.placeholder((batch, in_channel, in_height, in_width), name='A')\n",
    "    W = te.placeholder((num_filter, in_channel), name='W')\n",
    "    B = compute_conv2d(A, W, stride, padding)\n",
    "    s = schedule_conv2d(B)\n",
    "\n",
    "    s = s.normalize()\n",
    "    print(tvm.lower(s, [A, W, B], simple_mode=True))\n",
    "\n",
    "\n",
    "def test_conv2d_nchw():\n",
    "    verify_conv2d_nchw(batch=1, in_channel=128, in_height=16, in_width=16, num_filter=64, kernel=1, stride=1, padding=0)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_conv2d_nchw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "circular-graphic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvm-buildkernel",
   "language": "python",
   "name": "tvm-buildkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
